{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9dqiFgEvgGg"
      },
      "outputs": [],
      "source": [
        "# Colab-ready end-to-end project:\n",
        "# Advanced Time Series Forecasting with Attention-based Transformers\n",
        "# ================================================================\n",
        "# Run this cell in Google Colab. It's a single script that:\n",
        "# 1) Generates synthetic data (>=3 features, 5 years daily)\n",
        "# 2) Trains a decoder-only Transformer for autoregressive multi-step forecasting\n",
        "# 3) Trains an LSTM baseline\n",
        "# 4) Performs rolling-window (walk-forward) backtesting and computes RMSE and MAE\n",
        "# 5) Performs a small hyperparameter sweep (attention-related params)\n",
        "# 6) Saves models and writes a textual report (markdown)\n",
        "#\n",
        "# Notes:\n",
        "# - Requires PyTorch. Colab usually has torch installed; if not, run:\n",
        "#     !pip install -q torch torchvision\n",
        "# - This script is intentionally modular; tune Config to control runtime.\n",
        "# - For large searches / epochs, use a Colab GPU runtime.\n",
        "#\n",
        "# Author: ChatGPT (GPT-5 Thinking mini)\n",
        "# ------------------------------------------------\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, List, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# -----------------------\n",
        "# Config: adjust for Colab\n",
        "# -----------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    seed: int = 42\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # Data\n",
        "    start_date: str = \"2015-01-01\"\n",
        "    days: int = 5 * 365  # 5 years of daily data (approx)\n",
        "    n_features: int = 4   # at least 3; choose 4\n",
        "    freq: str = \"D\"\n",
        "    # Forecasting setup\n",
        "    lookback: int = 90    # days past used as input\n",
        "    horizon: int = 30     # days to forecast (multi-step)\n",
        "    # Model/hyperparams defaults (can be overridden in grid)\n",
        "    d_model: int = 64\n",
        "    n_heads: int = 4\n",
        "    num_layers: int = 2\n",
        "    dropout: float = 0.1\n",
        "    lr: float = 1e-4\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 12      # keep small by default; increase for better results\n",
        "    # Backtesting\n",
        "    backtest_folds: int = 6\n",
        "    # Hyperparameter sweep (small by default)\n",
        "    sweep: Dict[str, List[Any]] = None\n",
        "    # Paths\n",
        "    out_dir: str = \"./ts_transformer_results\"\n",
        "    # Misc\n",
        "    grad_clip: float = 1.0\n",
        "\n",
        "cfg = Config()\n",
        "cfg.sweep = {\n",
        "    \"num_layers\": [1, 2],\n",
        "    \"n_heads\": [2, 4],\n",
        "    \"d_model\": [32, 64],\n",
        "    \"dropout\": [0.05, 0.1]\n",
        "}\n",
        "\n",
        "# Create output dir\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------\n",
        "# reproducibility\n",
        "# -----------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "# -----------------------\n",
        "# Synthetic Data Generator\n",
        "# -----------------------\n",
        "def generate_synthetic_multivariate(start_date: str, days: int, n_features: int, freq: str = \"D\", seed: int = 42):\n",
        "    \"\"\"\n",
        "    Generate synthetic multivariate time series:\n",
        "      - base trend (linear)\n",
        "      - seasonal components (annual + weekly)\n",
        "      - feature-specific amplitudes and phase shifts\n",
        "      - cross-feature coupling (linear mixing)\n",
        "      - additive Gaussian noise\n",
        "    Returns pandas DataFrame with Date index and n_features columns.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    dates = pd.date_range(start=start_date, periods=days, freq=freq)\n",
        "    t = np.arange(days).astype(float)\n",
        "\n",
        "    # base trend\n",
        "    trend = 0.0005 * t  # small linear trend\n",
        "\n",
        "    # seasonal components\n",
        "    annual = np.sin(2 * np.pi * t / 365.25)\n",
        "    weekly = np.sin(2 * np.pi * t / 7.0)\n",
        "    monthly = np.sin(2 * np.pi * t / 30.0)\n",
        "\n",
        "    base = trend + 0.5 * annual + 0.2 * weekly + 0.1 * monthly\n",
        "\n",
        "    # feature-specific modulations and noise\n",
        "    features = []\n",
        "    for i in range(n_features):\n",
        "        amp = 0.8 + 0.4 * np.random.rand()\n",
        "        phase = 2 * np.pi * np.random.rand()\n",
        "        feature = amp * (base + 0.2 * np.sin(2 * np.pi * t / (365.25 / (1 + i * 0.2)) + phase))\n",
        "        features.append(feature)\n",
        "\n",
        "    X = np.vstack(features).T  # shape (days, n_features)\n",
        "\n",
        "    # Add short-term noise and heteroskedasticity\n",
        "    noise_scale = 0.05 + 0.05 * np.random.rand(n_features)\n",
        "    noise = np.random.randn(*X.shape) * noise_scale\n",
        "    X = X + noise * (1 + 0.3 * np.sin(2 * np.pi * t[:, None] / 365.25))\n",
        "\n",
        "    # Add linear cross-coupling\n",
        "    A = np.eye(n_features) * 0.8 + 0.2 * (np.random.rand(n_features, n_features) - 0.5)\n",
        "    X = X @ A.T\n",
        "\n",
        "    # Standardize each feature (zero mean, unit var)\n",
        "    mu = X.mean(axis=0)\n",
        "    sigma = X.std(axis=0)\n",
        "    X = (X - mu) / (sigma + 1e-8)\n",
        "\n",
        "    df = pd.DataFrame(X, index=dates, columns=[f\"f{i+1}\" for i in range(n_features)])\n",
        "    return df\n",
        "\n",
        "# Generate data\n",
        "df = generate_synthetic_multivariate(cfg.start_date, cfg.days, cfg.n_features, cfg.freq, cfg.seed)\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# -----------------------\n",
        "# Dataset & DataLoader\n",
        "# -----------------------\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Prepares sliding windows (lookback -> horizon) for training or evaluation.\n",
        "    Each sample contains:\n",
        "       - encoder_input: shape (lookback, n_features)\n",
        "       - decoder_input: shape (horizon, n_features) during training (teacher forcing)\n",
        "       - target: shape (horizon, n_features)\n",
        "    For Transformer decoder-only autoregressive training, we provide teacher forcing target sequence as decoder_input.\n",
        "    \"\"\"\n",
        "    def __init__(self, data: pd.DataFrame, lookback: int, horizon: int):\n",
        "        self.data_values = data.values.astype(np.float32)\n",
        "        self.lookback = lookback\n",
        "        self.horizon = horizon\n",
        "        self.n = len(data)\n",
        "        self.n_features = self.data_values.shape[1]\n",
        "        # valid start indices where both lookback and horizon fit\n",
        "        self.starts = np.arange(0, self.n - (lookback + horizon) + 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.starts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.starts[idx]\n",
        "        encoder = self.data_values[s: s + self.lookback]  # (L, F)\n",
        "        target = self.data_values[s + self.lookback: s + self.lookback + self.horizon]  # (H, F)\n",
        "        # decoder_input for teacher forcing: shift-right with start token = last value of encoder\n",
        "        # We'll provide decoder_input where first time step is last encoder value (or zeros). Simpler: prepend last encoder row.\n",
        "        start_token = encoder[-1:, :]  # (1, F)\n",
        "        decoder_input = np.concatenate([start_token, target[:-1]], axis=0)  # (H, F)\n",
        "        return {\n",
        "            \"encoder\": torch.from_numpy(encoder),           # (L, F)\n",
        "            \"decoder_in\": torch.from_numpy(decoder_input), # (H, F)\n",
        "            \"target\": torch.from_numpy(target)             # (H, F)\n",
        "        }\n",
        "\n",
        "# Utility to create loaders\n",
        "def make_dataloader_from_df(df: pd.DataFrame, lookback: int, horizon: int, batch_size: int, shuffle=True):\n",
        "    ds = TimeSeriesDataset(df, lookback, horizon)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "    return loader\n",
        "\n",
        "# -----------------------\n",
        "# Positional Encoding\n",
        "# -----------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 1000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:seq_len].unsqueeze(0).to(x.device)\n",
        "        return x\n",
        "\n",
        "# -----------------------\n",
        "# Transformer Decoder-only Model\n",
        "# -----------------------\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, n_features: int, d_model: int = 64, n_heads: int = 4, num_layers: int = 2,\n",
        "                 dropout: float = 0.1, lookback: int = 90, horizon: int = 30):\n",
        "        \"\"\"\n",
        "        Decoder-only architecture:\n",
        "          - Input encoder: embed past lookback sequence to d_model\n",
        "          - Decoder: autoregressive decoding for horizon steps using masked multi-head attention\n",
        "          - Predicts multivariate output each decoder time step (linear projection to n_features)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.d_model = d_model\n",
        "        self.lookback = lookback\n",
        "        self.horizon = horizon\n",
        "\n",
        "        # input embedding: project features -> d_model\n",
        "        self.input_proj = nn.Linear(n_features, d_model)\n",
        "        # decoder input proj (for decoder tokens)\n",
        "        self.dec_input_proj = nn.Linear(n_features, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len=max(lookback, horizon) + 10)\n",
        "\n",
        "        # Transformer decoder layers built from nn.TransformerDecoderLayer\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n",
        "                                                   dropout=dropout, activation='relu', batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # We'll use the encoder memory as the embedded encoder sequence\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_proj = nn.Linear(d_model, n_features)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, encoder_x, decoder_x, teacher_force_mask=None):\n",
        "        \"\"\"\n",
        "        encoder_x: (B, L, F)\n",
        "        decoder_x: (B, H, F)  - for training: teacher-forced decoder inputs\n",
        "        returns: preds (B, H, F)\n",
        "        \"\"\"\n",
        "        # Embed\n",
        "        enc = self.input_proj(encoder_x)   # (B, L, d_model)\n",
        "        dec = self.dec_input_proj(decoder_x)  # (B, H, d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        enc = self.pos_enc(enc)\n",
        "        dec = self.pos_enc(dec)\n",
        "\n",
        "        # Use transformer decoder with memory=enc\n",
        "        # Build causal mask for decoder self-attention (so token t can only see <= t)\n",
        "        H = dec.size(1)\n",
        "        device = dec.device\n",
        "        causal_mask = torch.triu(torch.ones((H, H), device=device) * float('-inf'), diagonal=1)\n",
        "        # The transformer API expects mask shape (tgt_len, tgt_len) or (batch_first=True -> (B, tgt_len, tgt_len) if provided per-batch)\n",
        "        memory = enc  # (B, L, d_model)\n",
        "        dec_out = self.transformer_decoder(tgt=dec, memory=memory, tgt_mask=causal_mask)  # (B, H, d_model)\n",
        "\n",
        "        out = self.output_proj(self.dropout(dec_out))  # (B, H, F)\n",
        "        return out\n",
        "\n",
        "    def generate_autoregressive(self, encoder_x, horizon: int):\n",
        "        \"\"\"\n",
        "        Autoregressive generation for inference.\n",
        "        encoder_x: (B, L, F)\n",
        "        Returns: preds (B, horizon, F)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        B = encoder_x.shape[0]\n",
        "        device = encoder_x.device\n",
        "\n",
        "        # Initialize decoder input with start token = last encoder step\n",
        "        last_enc = encoder_x[:, -1:, :]  # (B, 1, F)\n",
        "        preds = []\n",
        "        dec_inputs = last_enc  # (B, cur_H, F)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for t in range(horizon):\n",
        "                out = self.forward(encoder_x, dec_inputs)  # (B, cur_H, F) -> we only need last step\n",
        "                next_step = out[:, -1:, :]  # (B,1,F)\n",
        "                preds.append(next_step)\n",
        "                dec_inputs = torch.cat([dec_inputs, next_step], dim=1)\n",
        "        preds = torch.cat(preds, dim=1)  # (B, horizon, F)\n",
        "        return preds\n",
        "\n",
        "# -----------------------\n",
        "# LSTM Baseline\n",
        "# -----------------------\n",
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, n_features: int, hidden_size: int = 64, num_layers: int = 2, horizon: int = 30, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.horizon = horizon\n",
        "        self.rnn = nn.LSTM(input_size=n_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.proj = nn.Linear(hidden_size, n_features)\n",
        "        # We'll produce horizon predictions autoregressively using the projected hidden state fed back - simpler: produce horizon in one go by repeating the last hidden state\n",
        "        # (Alternative: seq2seq LSTM decoder; kept simpler here)\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, F)\n",
        "        out, (h_n, c_n) = self.rnn(x)  # out: (B, L, H)\n",
        "        last = out[:, -1, :]  # (B, H)\n",
        "        # Repeat last hidden state horizon times and project\n",
        "        repeated = last.unsqueeze(1).repeat(1, self.horizon, 1)  # (B, H, hidden)\n",
        "        out = self.proj(repeated)  # (B, horizon, n_features)\n",
        "        return out\n",
        "\n",
        "# -----------------------\n",
        "# Loss and Metrics\n",
        "# -----------------------\n",
        "def mae(pred, target):\n",
        "    return torch.mean(torch.abs(pred - target)).item()\n",
        "\n",
        "def rmse(pred, target):\n",
        "    return torch.sqrt(torch.mean((pred - target) ** 2)).item()\n",
        "\n",
        "# -----------------------\n",
        "# Training loop for Transformer\n",
        "# -----------------------\n",
        "def train_transformer(model: TimeSeriesTransformer, train_loader, val_loader, cfg: Config, epochs: int, lr: float, device: str, grad_clip: float):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch}/{epochs}\", leave=False)\n",
        "        for batch in pbar:\n",
        "            enc = batch[\"encoder\"].to(device)    # (B, L, F)\n",
        "            dec_in = batch[\"decoder_in\"].to(device)  # (B, H, F)\n",
        "            target = batch[\"target\"].to(device)  # (B, H, F)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(enc, dec_in)  # teacher forcing\n",
        "            loss = criterion(preds, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": float(np.mean(train_losses))})\n",
        "        avg_train = float(np.mean(train_losses))\n",
        "        # Validation (compute loss using autoregressive generation for proper evaluation)\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                enc = batch[\"encoder\"].to(device)\n",
        "                target = batch[\"target\"].to(device)\n",
        "                preds = model.generate_autoregressive(enc, model.horizon)\n",
        "                loss = criterion(preds, target)\n",
        "                val_losses.append(loss.item())\n",
        "        avg_val = float(np.mean(val_losses)) if val_losses else float(\"nan\")\n",
        "        history[\"train_loss\"].append(avg_train)\n",
        "        history[\"val_loss\"].append(avg_val)\n",
        "        print(f\"Epoch {epoch} train_loss={avg_train:.6f} val_loss={avg_val:.6f}\")\n",
        "    return history\n",
        "\n",
        "# Training loop for LSTM baseline\n",
        "def train_lstm(model: LSTMForecaster, train_loader, val_loader, cfg: Config, epochs: int, lr: float, device: str, grad_clip: float):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for batch in train_loader:\n",
        "            enc = batch[\"encoder\"].to(device)\n",
        "            target = batch[\"target\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(enc)\n",
        "            loss = criterion(preds, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "        avg_train = float(np.mean(train_losses))\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                enc = batch[\"encoder\"].to(device)\n",
        "                target = batch[\"target\"].to(device)\n",
        "                preds = model(enc)\n",
        "                loss = criterion(preds, target)\n",
        "                val_losses.append(loss.item())\n",
        "        avg_val = float(np.mean(val_losses)) if val_losses else float(\"nan\")\n",
        "        history[\"train_loss\"].append(avg_train)\n",
        "        history[\"val_loss\"].append(avg_val)\n",
        "        print(f\"LSTM Epoch {epoch} train_loss={avg_train:.6f} val_loss={avg_val:.6f}\")\n",
        "    return history\n",
        "\n",
        "# -----------------------\n",
        "# Rolling-window backtesting (walk-forward)\n",
        "# -----------------------\n",
        "def rolling_backtest(df: pd.DataFrame, model_type: str, cfg: Config, model_kwargs: dict, training_epochs: int = None):\n",
        "    \"\"\"\n",
        "    Perform expanding/rolling window backtesting:\n",
        "      - Split the dataset into backtest_folds sequential folds.\n",
        "      - For each fold:\n",
        "           train on data up to fold_start\n",
        "           validate/test on next horizon (single fold horizon length = cfg.horizon)\n",
        "      - Accumulate RMSE and MAE per fold and return summary.\n",
        "\n",
        "    model_type: 'transformer' or 'lstm'\n",
        "    model_kwargs: dict used to initialize model\n",
        "    \"\"\"\n",
        "    n = len(df)\n",
        "    fold_horizon = cfg.horizon\n",
        "    # We'll set fold_starts to be equally spaced near the end of the series\n",
        "    # Last fold_end must be <= n\n",
        "    # Use backtest_folds folds: for i in 0..folds-1 -> train_end = initial_train + i * step\n",
        "    # Simplest: define test_starts at the last backtest_folds positions separated by fold_horizon\n",
        "    test_starts = [n - (cfg.backtest_folds - i) * fold_horizon for i in range(cfg.backtest_folds)]\n",
        "    fold_metrics = []\n",
        "    models = []\n",
        "    for i, test_start in enumerate(test_starts):\n",
        "        train_end = test_start  # exclusive: train up to index train_end-1\n",
        "        train_df = df.iloc[:train_end]\n",
        "        test_df = df.iloc[test_start: test_start + fold_horizon + cfg.lookback]  # need lookback + horizon for dataset\n",
        "        # Build datasets: train on all windows that fully fit inside train_df\n",
        "        train_loader = make_dataloader_from_df(train_df, cfg.lookback, cfg.horizon, batch_size=cfg.batch_size, shuffle=True)\n",
        "        val_loader = make_dataloader_from_df(test_df, cfg.lookback, cfg.horizon, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "        # initialize model\n",
        "        if model_type == 'transformer':\n",
        "            model = TimeSeriesTransformer(n_features=cfg.n_features, lookback=cfg.lookback, horizon=cfg.horizon, **model_kwargs)\n",
        "            if training_epochs is None:\n",
        "                training_epochs = cfg.epochs\n",
        "            _ = train_transformer(model, train_loader, val_loader, cfg, training_epochs, lr=cfg.lr, device=cfg.device, grad_clip=cfg.grad_clip)\n",
        "            # Evaluate on val_loader using autoregressive generation and compute metrics\n",
        "            model = model.to(cfg.device)\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    enc = batch[\"encoder\"].to(cfg.device)\n",
        "                    target = batch[\"target\"].to(cfg.device)\n",
        "                    preds = model.generate_autoregressive(enc, cfg.horizon)\n",
        "                    all_preds.append(preds.cpu())\n",
        "                    all_targets.append(target.cpu())\n",
        "            preds = torch.cat(all_preds, dim=0)\n",
        "            targets = torch.cat(all_targets, dim=0)\n",
        "        elif model_type == 'lstm':\n",
        "            model = LSTMForecaster(n_features=cfg.n_features, horizon=cfg.horizon, **model_kwargs)\n",
        "            if training_epochs is None:\n",
        "                training_epochs = cfg.epochs\n",
        "            _ = train_lstm(model, train_loader, val_loader, cfg, training_epochs, lr=cfg.lr, device=cfg.device, grad_clip=cfg.grad_clip)\n",
        "            model = model.to(cfg.device)\n",
        "            model.eval()\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    enc = batch[\"encoder\"].to(cfg.device)\n",
        "                    target = batch[\"target\"].to(cfg.device)\n",
        "                    preds = model(enc)\n",
        "                    all_preds.append(preds.cpu())\n",
        "                    all_targets.append(target.cpu())\n",
        "            preds = torch.cat(all_preds, dim=0)\n",
        "            targets = torch.cat(all_targets, dim=0)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model_type\")\n",
        "\n",
        "        # Compute RMSE and MAE aggregated across batches and horizon\n",
        "        fold_rmse = rmse(preds, targets)\n",
        "        fold_mae = mae(preds, targets)\n",
        "        fold_metrics.append({\"fold\": i, \"test_start_index\": test_start, \"rmse\": fold_rmse, \"mae\": fold_mae})\n",
        "        print(f\"Fold {i} {model_type} RMSE={fold_rmse:.6f} MAE={fold_mae:.6f}\")\n",
        "        models.append(model)\n",
        "    # aggregate\n",
        "    agg = {\n",
        "        \"rmse_mean\": float(np.mean([m[\"rmse\"] for m in fold_metrics])),\n",
        "        \"rmse_std\": float(np.std([m[\"rmse\"] for m in fold_metrics])),\n",
        "        \"mae_mean\": float(np.mean([m[\"mae\"] for m in fold_metrics])),\n",
        "        \"mae_std\": float(np.std([m[\"mae\"] for m in fold_metrics])),\n",
        "        \"folds\": fold_metrics\n",
        "    }\n",
        "    return agg, models\n",
        "\n",
        "# -----------------------\n",
        "# Hyperparameter sweep for Transformer (focus on attention params)\n",
        "# -----------------------\n",
        "def hyperparam_sweep_transformer(df: pd.DataFrame, cfg: Config):\n",
        "    \"\"\"\n",
        "    Iterate over cfg.sweep and evaluate via backtesting (note: this trains multiple models).\n",
        "    Keep the best config by rmse_mean.\n",
        "    Returns a results list and best config.\n",
        "    \"\"\"\n",
        "    sweep_keys = list(cfg.sweep.keys())\n",
        "    from itertools import product\n",
        "    combos = list(product(*[cfg.sweep[k] for k in sweep_keys]))\n",
        "    results = []\n",
        "    best = None\n",
        "    best_rmse = float(\"inf\")\n",
        "    print(f\"Running hyperparam sweep: {len(combos)} combinations (this will take time).\")\n",
        "    for combo in combos:\n",
        "        params = dict(zip(sweep_keys, combo))\n",
        "        print(\"Evaluating config:\", params)\n",
        "        model_kwargs = {\n",
        "            \"d_model\": params.get(\"d_model\", cfg.d_model),\n",
        "            \"n_heads\": params.get(\"n_heads\", cfg.n_heads),\n",
        "            \"num_layers\": params.get(\"num_layers\", cfg.num_layers),\n",
        "            \"dropout\": params.get(\"dropout\", cfg.dropout),\n",
        "        }\n",
        "        agg, _ = rolling_backtest(df, model_type='transformer', cfg=cfg, model_kwargs=model_kwargs, training_epochs=max(3, cfg.epochs//2))\n",
        "        res = {\"params\": params, **agg}\n",
        "        results.append(res)\n",
        "        if agg[\"rmse_mean\"] < best_rmse:\n",
        "            best_rmse = agg[\"rmse_mean\"]\n",
        "            best = res\n",
        "        print(f\"Combo RMSE_mean = {agg['rmse_mean']:.6f}\")\n",
        "    return results, best\n",
        "\n",
        "# -----------------------\n",
        "# Run everything\n",
        "# -----------------------\n",
        "def run_full_pipeline(df: pd.DataFrame, cfg: Config):\n",
        "    # Split into train/validation/test for initial quick training (not backtesting)\n",
        "    n = len(df)\n",
        "    train_frac = 0.7\n",
        "    val_frac = 0.1\n",
        "    train_end = int(n * train_frac)\n",
        "    val_end = int(n * (train_frac + val_frac))\n",
        "\n",
        "    train_df = df.iloc[:train_end]\n",
        "    val_df = df.iloc[train_end:val_end]\n",
        "    test_df = df.iloc[val_end:]\n",
        "\n",
        "    print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))\n",
        "\n",
        "    # Quick baseline run: train one Transformer with default cfg\n",
        "    default_model_kwargs = {\"d_model\": cfg.d_model, \"n_heads\": cfg.n_heads, \"num_layers\": cfg.num_layers, \"dropout\": cfg.dropout}\n",
        "    print(\"Backtesting Transformer with default params...\")\n",
        "    transformer_agg, transformer_models = rolling_backtest(df, model_type='transformer', cfg=cfg, model_kwargs=default_model_kwargs, training_epochs=cfg.epochs)\n",
        "\n",
        "    print(\"Training LSTM baseline with comparable capacity...\")\n",
        "    # Set LSTM hidden size roughly comparable to d_model\n",
        "    lstm_kwargs = {\"hidden_size\": cfg.d_model, \"num_layers\": cfg.num_layers, \"dropout\": cfg.dropout}\n",
        "    lstm_agg, lstm_models = rolling_backtest(df, model_type='lstm', cfg=cfg, model_kwargs=lstm_kwargs, training_epochs=cfg.epochs)\n",
        "\n",
        "    # Hyperparameter sweep (small)\n",
        "    sweep_results, best = hyperparam_sweep_transformer(df, cfg)\n",
        "\n",
        "    # Save results to files\n",
        "    import json\n",
        "    with open(os.path.join(cfg.out_dir, \"transformer_agg.json\"), \"w\") as f:\n",
        "        json.dump(transformer_agg, f, indent=2)\n",
        "    with open(os.path.join(cfg.out_dir, \"lstm_agg.json\"), \"w\") as f:\n",
        "        json.dump(lstm_agg, f, indent=2)\n",
        "    with open(os.path.join(cfg.out_dir, \"sweep_results.json\"), \"w\") as f:\n",
        "        json.dump(sweep_results, f, indent=2)\n",
        "    if best:\n",
        "        with open(os.path.join(cfg.out_dir, \"best_config.json\"), \"w\") as f:\n",
        "            json.dump(best, f, indent=2)\n",
        "\n",
        "    # Save a short markdown report (text-based deliverable)\n",
        "    report_md = build_report(transformer_agg, lstm_agg, best, sweep_results, cfg)\n",
        "    with open(os.path.join(cfg.out_dir, \"report.md\"), \"w\") as f:\n",
        "        f.write(report_md)\n",
        "\n",
        "    print(f\"Outputs saved to {cfg.out_dir}\")\n",
        "    return transformer_agg, lstm_agg, sweep_results, best\n",
        "\n",
        "# -----------------------\n",
        "# Build textual report\n",
        "# -----------------------\n",
        "def build_report(transformer_agg, lstm_agg, best_config, sweep_results, cfg: Config) -> str:\n",
        "    now = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
        "    md = []\n",
        "    md.append(f\"# Project Report: Advanced Time Series Forecasting with Attention-based Transformers\\n\")\n",
        "    md.append(f\"Generated: {now}\\n\")\n",
        "    md.append(\"## Project Summary\\n\")\n",
        "    md.append(\"This project implements a decoder-only Transformer architecture for multi-step time series forecasting, \"\n",
        "              \"compares it to an LSTM baseline, performs rolling-window backtesting, and conducts a targeted hyperparameter sweep focusing on attention mechanism hyperparameters (layer depth, head count, dropout, embedding dimension).\\n\")\n",
        "    md.append(\"## Dataset\\n\")\n",
        "    md.append(f\"- Synthetic multivariate dataset with {cfg.n_features} features, {cfg.days} days from {cfg.start_date}.\\n\")\n",
        "    md.append(\"Generation details: base trend, annual/weekly/monthly seasonality, correlated features with additive heteroskedastic noise. Data is standardized per feature.\\n\")\n",
        "\n",
        "    md.append(\"## Model Architecture (Transformer)\\n\")\n",
        "    md.append(\"Key choices:\\n\")\n",
        "    md.append(\"- Decoder-only Transformer: we embed both encoder (past lookback) and decoder (teacher-forced target) tokens to the same model dimension and use a standard TransformerDecoder with causal masking. This is a natural fit for autoregressive multi-step forecasting. \\n\")\n",
        "    md.append(\"- Input projection: each multivariate timestep (vector of features) is linearly projected to a d_model embedding. Positional encodings are added.\\n\")\n",
        "    md.append(\"- Causal masking in the decoder ensures autoregressive conditioning (token t only sees tokens <= t).\\n\")\n",
        "    md.append(\"- Final linear projection maps the decoder output back to the original multivariate feature space.\\n\")\n",
        "\n",
        "    md.append(\"## Training and Optimization\\n\")\n",
        "    md.append(f\"- Loss: MSE (L2). Metrics reported: RMSE and MAE.\\n- Optimizer: Adam with learning rate {cfg.lr}.\\n- Gradient clipping: {cfg.grad_clip} to stabilize training.\\n- Teacher forcing used during training for the decoder; autoregressive generation used for validation/testing to simulate real forecasting.\\n\")\n",
        "\n",
        "    md.append(\"## Backtesting (Walk-forward validation)\\n\")\n",
        "    md.append(f\"- Performed {cfg.backtest_folds} sequential folds. For each fold the model is trained using all data up to that fold, then evaluated on the next horizon ({cfg.horizon} days). Metrics are aggregated across folds (mean/std).\\n\")\n",
        "\n",
        "    md.append(\"## Results Summary\\n\")\n",
        "    md.append(\"Transformer (default params):\\n\")\n",
        "    md.append(f\"- RMSE mean: {transformer_agg['rmse_mean']:.6f}, RMSE std: {transformer_agg['rmse_std']:.6f}\\n\")\n",
        "    md.append(f\"- MAE mean: {transformer_agg['mae_mean']:.6f}, MAE std: {transformer_agg['mae_std']:.6f}\\n\")\n",
        "\n",
        "    md.append(\"LSTM baseline:\\n\")\n",
        "    md.append(f\"- RMSE mean: {lstm_agg['rmse_mean']:.6f}, RMSE std: {lstm_agg['rmse_std']:.6f}\\n\")\n",
        "    md.append(f\"- MAE mean: {lstm_agg['mae_mean']:.6f}, MAE std: {lstm_agg['mae_std']:.6f}\\n\")\n",
        "\n",
        "    md.append(\"Hyperparameter sweep (attention-focused):\\n\")\n",
        "    if best_config:\n",
        "        md.append(f\"- Best config (by RMSE mean): {best_config['params']}\\n\")\n",
        "        md.append(f\"- Best RMSE mean: {best_config['rmse_mean']:.6f}\\n\")\n",
        "    else:\n",
        "        md.append(\"- Sweep not completed or empty results.\\n\")\n",
        "\n",
        "    md.append(\"## Interpretability & Attention\\n\")\n",
        "    md.append(\"Although visualizations are not included here, attention weights can provide interpretability gains: \\n\")\n",
        "    md.append(\"- By inspecting decoder self-attention and encoder->decoder attention matrices, one can see which past timesteps (and which features via cross-attention patterns after appropriate aggregation) the model attends to when forecasting particular future timesteps.\\n\")\n",
        "    md.append(\"- Attention can reveal seasonality alignment (e.g., strong attention to ~7-day lags for weekly patterns or ~365-day for annual patterns) and feature coupling patterns.\\n\")\n",
        "    md.append(\"- For multivariate sequences, attention can be aggregated per feature (e.g., compute sum of weights associated with tokens that predominantly represent feature i) to analyze cross-feature influences.\\n\")\n",
        "    md.append(\"Caveats: Attention weights are not guaranteed to be a perfect explanation of causality; they are one interpretable lens.\\n\")\n",
        "\n",
        "    md.append(\"## How to run / extend\\n\")\n",
        "    md.append(\"- Increase epochs or broaden the sweep grid for improved performance.\\n\")\n",
        "    md.append(\"- To inspect attention weights for interpretability, modify the Transformer to return the attention matrices from its MultiheadAttention layers. Then aggregate and plot them for selected test cases.\\n\")\n",
        "    md.append(\"## Files produced\\n\")\n",
        "    md.append(f\"- JSON summaries: transformer_agg.json, lstm_agg.json, sweep_results.json, best_config.json\\n- report.md (this file)\\n\")\n",
        "\n",
        "    md.append(\"\\n---\\nEnd of report.\\n\")\n",
        "    return \"\\n\".join(md)\n",
        "\n",
        "# -----------------------\n",
        "# Execute pipeline\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Placeholders: run the full pipeline; this will take some time depending on cfg settings.\n",
        "    print(f\"Device: {cfg.device}; Running full pipeline with lookback={cfg.lookback}, horizon={cfg.horizon}, epochs={cfg.epochs}\")\n",
        "    transformer_agg, lstm_agg, sweep_results, best = run_full_pipeline(df, cfg)\n",
        "    print(\"Done. Check the output directory for saved models and report.\")\n"
      ]
    }
  ]
}